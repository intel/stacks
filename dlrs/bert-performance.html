<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>State-of-the-art BERT Fine-tune training and Inference &mdash; System Stacks for Linux* OS  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep Learning Reference Stack with Tensorflow and Intel® oneAPI Deep Neural Network Library (oneDNN)" href="tensorflow/README.html" />
    <link rel="prev" title="Deep Learning Reference Stack Guide" href="dlrs.html" />
    <link href="../_static/css/custom.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> System Stacks for Linux* OS
            <img src="../_static/stacks_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../README.html">Stacks containers have been deprecated and please switch to oneapi based containers, you can find oneapi containers at this link :  https://hub.docker.com/u/intel</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../README.html#system-stacks-for-linux-os">System Stacks for Linux* OS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../README.html#contributing">Contributing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../README.html#security-issues">Security Issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="../README.html#mailing-list">Mailing List</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Deep Learning Reference Stack</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="index.html#dlrs-release-announcement-and-performance-reports">DLRS Release Announcement and Performance Reports</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html#dlrs-guides">DLRS Guides</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="dlrs.html">Deep Learning Reference Stack Guide</a><ul>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#releases">Releases</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#tensorflow-single-and-multi-node-benchmarks">TensorFlow single and multi-node benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#pytorch-single-and-multi-node-benchmarks">PyTorch single and multi-node benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#tensorflow-training-tfjob-with-kubeflow-and-dlrs">TensorFlow Training (TFJob) with Kubeflow and DLRS</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#pytorch-training-pytorch-job-with-kubeflow-and-dlrs">PyTorch Training (PyTorch Job) with Kubeflow and DLRS</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#working-with-horovod-and-openmpi">Working with Horovod* and OpenMPI*</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#using-transformers-for-natural-language-processing">Using Transformers* for Natural Language Processing</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#using-the-openvino-model-optimizer">Using the OpenVINO™ Model Optimizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#using-the-openvino-toolkit-inference-engine">Using the OpenVINO™ toolkit Inference Engine</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#using-seldon-and-openvino-model-server-with-the-deep-learning-reference-stack">Using Seldon and OpenVINO™ model server with the Deep Learning Reference Stack</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#use-jupyter-notebook">Use Jupyter Notebook</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#uninstallation">Uninstallation</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#compiling-aixprt-with-openmp-on-dlrs">Compiling AIXPRT with OpenMP on DLRS</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#using-the-intel-vtune-profiler-with-dlrs-containers">Using the Intel® VTune™ Profiler with DLRS Containers</a></li>
<li class="toctree-l4"><a class="reference internal" href="dlrs.html#related-resources">Related Resources</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">State-of-the-art BERT Fine-tune training and Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#recommended-hardware">Recommended Hardware</a></li>
<li class="toctree-l4"><a class="reference internal" href="#required-software">Required Software</a></li>
<li class="toctree-l4"><a class="reference internal" href="#steps">Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-bert-fine-tune-training-with-the-squad-1-1-data-set">Run BERT Fine-tune training with the Squad 1.1 data set</a></li>
<li class="toctree-l4"><a class="reference internal" href="#notices-and-disclaimers">NOTICES AND DISCLAIMERS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#dlrs-releases">DLRS Releases</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#dlrs-with-tensorflow">DLRS with TensorFlow*</a><ul>
<li class="toctree-l4"><a class="reference internal" href="tensorflow/README.html">Deep Learning Reference Stack with Tensorflow and Intel® oneAPI Deep Neural Network Library (oneDNN)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dlrs-with-tensorflow-serving">DLRS with TensorFlow Serving*</a><ul>
<li class="toctree-l4"><a class="reference internal" href="serving/README.html">Build instructions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dlrs-with-pytorch">DLRS with PyTorch*</a><ul>
<li class="toctree-l4"><a class="reference internal" href="pytorch/README.html">Deep Learning Reference Stack with Pytorch and Intel® oneAPI Deep Neural Network Library (oneDNN)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dlrs-ml-compiler">DLRS ML Compiler</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ml-compiler/README.html">Stacks Deep Learning Compiler</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dsrs/index.html">Data Services Reference Stack</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsrs/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../dsrs/README.html">Data Services Reference Stack (DSRS) Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dsrs/terms_of_use.html">Data Services Reference Stack Terms of Use</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dsrs/index.html#memcached-versions">memcached* versions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../dsrs/memcached/README.html">Data Services Reference Stack(DSRS) - Memcached*</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../dsrs/index.html#redis-versions">Redis* versions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../dsrs/redis/README.html">Data Services Reference Stack(DSRS) - Redis*</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mers/index.html">Media Reference Stack</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../mers/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../mers/README.html">Media Reference Stack</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/README.html#mers-on-intel-onecontainer-portal">MeRS on Intel® oneContainer Portal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/README.html#source-code">Source Code</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/README.html#reporting-security-issues">Reporting Security Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/README.html#supported-platforms-and-media-codecs">Supported Platforms and Media Codecs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mers/releasenotes.html">Media Reference Stack Release Notes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/releasenotes.html#the-media-reference-stack">The Media Reference Stack</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/releasenotes.html#licensing">Licensing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/releasenotes.html#the-media-reference-stack-licenses">The Media Reference Stack licenses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/releasenotes.html#disclaimer">Disclaimer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/releasenotes.html#source-code">Source code</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/releasenotes.html#contributing-to-the-media-reference-stack">Contributing to the Media Reference Stack</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/releasenotes.html#reporting-security-issues">Reporting Security Issues</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mers/terms_of_use.html">Media Reference Stack Terms of Use</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mers/CONTRIBUTING.html">Contributing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/CONTRIBUTING.html#pull-request-process">Pull Request Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CONTRIBUTING.html#code-of-conduct">Code of Conduct</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mers/AUTHORS.html">Media Reference Stack Authors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mers/LICENSES.html">Licenses</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/LICENSES.html#mers-licenses">MeRS licenses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/LICENSES.html#disclaimer">Disclaimer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mers/index.html#guide">Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../mers/mers.html">Media Reference Stack Guide</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/mers.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/mers.html#releases">Releases</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/mers.html#get-the-pre-built-mers-container-image">Get the pre-built MeRS container image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/mers.html#build-the-mers-container-image-from-source">Build the MeRS container image from source</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/mers.html#use-the-mers-container-image">Use the MeRS container image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/mers.html#add-aom-support">Add AOM support</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mers/index.html#ubuntu-releases">Ubuntu* Releases</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../mers/ubuntu/INSTALL.html">Media Reference Stack - Ubuntu*</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/ubuntu/INSTALL.html#building-container-image">Building container image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/ubuntu/INSTALL.html#getting-mers-pre-built-image">Getting MeRS pre-built image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/ubuntu/INSTALL.html#running-the-media-container">Running the Media Container</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/ubuntu/INSTALL.html#run-examples">Run examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/ubuntu/INSTALL.html#reporting-security-issues">Reporting Security Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/ubuntu/INSTALL.html#legal-notice">LEGAL NOTICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mers/NEWS.html">Changes for <code class="docutils literal notranslate"><span class="pre">v0.4.0</span></code> :</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mers/NEWS.html#changes-for-v0-3-0">Changes for <code class="docutils literal notranslate"><span class="pre">v0.3.0</span></code> :</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mers/NEWS.html#changes-for-v0-2-0">Changes for <code class="docutils literal notranslate"><span class="pre">v0.2.0</span></code> :</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mers/NEWS.html#changes-for-v0-1-0">Changes for <code class="docutils literal notranslate"><span class="pre">v0.1.0</span></code> :</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mers/BUGS.html">Known Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/BUGS.html#mers-v0-4-0">MeRS <code class="docutils literal notranslate"><span class="pre">v0.4.0</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/BUGS.html#mers-v0-3-0">MeRS <code class="docutils literal notranslate"><span class="pre">v0.3.0</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/BUGS.html#mers-v0-2-0">MeRS <code class="docutils literal notranslate"><span class="pre">v0.2.0</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/BUGS.html#mers-v0-1-0">MeRS <code class="docutils literal notranslate"><span class="pre">v0.1.0</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mers/CHANGELOG.html">0.4.0 (2021-04-19)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#functionality-changes">Functionality Changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#deprecated-features">Deprecated Features</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#new-features">New Features</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#refactors">Refactors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mers/CHANGELOG.html#changelog-v0-3-0-2020-11-18">Changelog v0.3.0 (2020-11-18)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#id2">Functionality Changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#chores">Chores</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#documentation-changes">Documentation Changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#id3">New Features</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#bug-fixes">Bug Fixes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#id4">Refactors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#removed-features">Removed Features</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#security">Security</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#id5">0.2.0 (2020-04-14)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/CHANGELOG.html#id6">0.1.0 (2019-10-31)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mers/index.html#deprecated-releases">Deprecated Releases</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../mers/deprecated/clearlinux/INSTALL.html">Media Reference Stack - Clear Linux* OS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mers/deprecated/clearlinux/INSTALL.html#building-locally">Building Locally</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/deprecated/clearlinux/INSTALL.html#pulling-from-docker-hub">Pulling from Docker Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/deprecated/clearlinux/INSTALL.html#running-the-media-container">Running the Media Container</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mers/deprecated/clearlinux/INSTALL.html#run-examples">Run examples</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../hpcrs/index.html">High Performance Computing Reference Stack</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../hpcrs/index.html#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../hpcrs/README.html">High Performance Compute Reference Stack</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/README.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/README.html#releases">Releases</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/README.html#stack-features">Stack features</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/README.html#get-the-pre-built-hpcrs-container-image">Get the pre-built HPCRS container image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/README.html#build-the-hpcrs-container-image-from-source">Build the HPCRS container image from source</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/README.html#use-the-hpcrs-container-image">Use the HPCRS container image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/README.html#convert-the-hpcrs-image-to-a-singularity-image">Convert the HPCRS image to a Singularity image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/README.html#reporting-security-issues">Reporting Security Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/README.html#legal-notice">LEGAL NOTICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../hpcrs/NEWS.html">Release notes for HPCRS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/NEWS.html#release-v0-3-0">Release <code class="docutils literal notranslate"><span class="pre">v0.3.0</span></code> :</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../hpcrs/terms_of_use.html">High Performance Computing Reference Stack Terms of Use</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../hpcrs/index.html#guides">Guides</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../hpcrs/d2s/README.html">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/d2s/README.html#version-compatibility-verified">Version compatibility verified</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/d2s/README.html#singularity">Singularity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/d2s/README.html#charliecloud">Charliecloud</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../hpcrs/d2s/README.html#d2s-a-tool-to-convert-docker-images-to-singularity-images-or-charliecloud-directories">d2s - A tool to convert Docker images to Singularity images or Charliecloud directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/d2s/README.html#getting-d2s">Getting d2s</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/d2s/README.html#converting-to-a-singularity-image">Converting to a Singularity Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/d2s/README.html#converting-to-a-charliecloud-directory">Converting to a Charliecloud directory</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../hpcrs/docs/FAQ.html">HPCRS Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html">HPCRS Tutorial – Creating an Environment for Running Workloads</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#hardware-configuration">Hardware Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#software-prerequisites">Software Prerequisites</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#software-configuration">Software Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#configuring-the-kubernetes-master">Configuring the Kubernetes master</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#add-and-build-qe">Add and Build QE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#run-qe-on-the-hpcrs-image">Run  QE on the HPCRS image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#pytorch-benchmarks">PyTorch benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#using-dcp">Using DCP++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#using-spack-to-list-available-recipes">Using Spack* to list available recipes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../hpcrs/docs/hpcrs_tutorial.html#hpcrs-and-the-intel-vtune-profiler">HPCRS and the Intel® VTune™ Profiler</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../oneContainer/index.html">oneContainer Resources</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../oneContainer/index.html#onecontainer-api">oneContainer API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../oneContainer/index.html#onecontainer-templates">oneContainer Templates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../oneContainer/index.html#onecontainer-cloud-tool">oneContainer Cloud Tool</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../whitepapers/index.html">System Stacks Whitepapers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../whitepapers/index.html#identify-galaxies-using-the-deep-learning-reference-stack">Identify Galaxies Using the Deep Learning Reference Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../whitepapers/index.html#github-issue-classification-utilizing-the-end-to-end-system-stacks">GitHub* Issue Classification Utilizing the End-to-End System Stacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../whitepapers/index.html#using-ai-to-help-save-lives">Using AI to Help Save Lives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../whitepapers/index.html#state-of-the-art-bert-fine-tune-training-and-inference-on-3rd-gen-intel-xeon-scalable-processors-with-the-intel-deep-learning-reference-stack">State-of-the-art BERT Fine-tune Training and Inference on 3rd Gen Intel® Xeon® Scalable processors with the Intel Deep Learning Reference Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../whitepapers/index.html#pix2pix-utilizing-the-deep-learning-reference-stack">Pix2Pix: Utilizing the Deep Learning Reference Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../whitepapers/index.html#next-generation-hybrid-cloud-data-analytics-solution">Next-Generation Hybrid Cloud Data Analytics Solution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../whitepapers/index.html#deploying-machine-learning-models-with-dlrs-and-tensorflow-serving">Deploying Machine Learning Models with DLRS and TensorFlow* Serving</a></li>
<li class="toctree-l2"><a class="reference internal" href="../whitepapers/index.html#performance-models-in-runway-ml-with-the-deep-learning-reference-stack">Performance Models in Runway ML with the Deep Learning Reference Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../whitepapers/index.html#deep-learning-functions-as-a-service">Deep Learning Functions as a Service</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../perf.html">Performance and Benchmarks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../perf.html#deep-learning-reference-stack">Deep Learning Reference Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf.html#high-performance-computing-reference-stack">High Performance Computing Reference Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf.html#data-services-reference-stack">Data Services Reference Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf.html#media-reference-stack">Media Reference Stack</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/stacks-usecase">Real World Use Cases</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/stacks">Project GitHub repository</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">System Stacks for Linux* OS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Deep Learning Reference Stack</a> &raquo;</li>
      <li>State-of-the-art BERT Fine-tune training and Inference</li>

  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="state-of-the-art-bert-fine-tune-training-and-inference">
<span id="bert-performance"></span><h1>State-of-the-art BERT Fine-tune training and Inference<a class="headerlink" href="#state-of-the-art-bert-fine-tune-training-and-inference" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id1">Overview</a></p></li>
<li><p><a class="reference internal" href="#recommended-hardware" id="id2">Recommended Hardware</a></p></li>
<li><p><a class="reference internal" href="#required-software" id="id3">Required Software</a></p></li>
<li><p><a class="reference internal" href="#steps" id="id4">Steps</a></p></li>
<li><p><a class="reference internal" href="#run-bert-fine-tune-training-with-the-squad-1-1-data-set" id="id5">Run BERT Fine-tune training with the Squad 1.1 data set</a></p></li>
<li><p><a class="reference internal" href="#notices-and-disclaimers" id="id6">NOTICES AND DISCLAIMERS</a></p></li>
</ul>
</div>
<section id="overview">
<h2><a class="toc-backref" href="#id1">Overview</a><a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Driven by real-life use cases ranging from medical diagnostics to financial fraud detection, deep learning’s neural networks are growing in size and complexity as more and more data is available to be consumed. This influx of data allows for more accuracy in data scoring, which can result in better AI models, but presents challenges to compute performance.  To  help keep up with the computational power required to run the deep learning workloads,  Google came up with the BFLOAT16 format to increase performance on their Tensor Processing Units (TPUs).</p>
<p>BFLOAT16 uses one bit for sign, eight for exponent, and seven for mantissa. Due to its dynamic range, it can be used to represent gradients directly without the need for loss scaling. It has been shown that the BFLOAT16 format works as well as the FP32 format while delivering increased performance and reducing memory usage.</p>
<p>Intel’s 3rd Gen Intel® Xeon® Scalable processor, featuring Intel® Deep Learning Boost, is the first general-purpose x86 CPU to support the BFLOAT16 format.</p>
<p>The latest Deep Learning Reference Stack (DLRS) 7.0 release integrated Intel optimized TensorFlow, which enables BFLOAT16 support for servers with the 3rd Gen Intel® Xeon® Scalable processor. Now AI developers can quickly develop, iterate and run BFLOAT16 models directly by utilizing the DLRS stack.</p>
<p>In this guide we walk through a solution to set up your infrastructure and deploy a Bidirectional Encoder Representations from Transformers (BERT) fine tune training and inference workload using the DLRS containers from Intel.</p>
</section>
<section id="recommended-hardware">
<h2><a class="toc-backref" href="#id2">Recommended Hardware</a><a class="headerlink" href="#recommended-hardware" title="Permalink to this headline"></a></h2>
<p>We recommend a 3rd Generation Intel Xeon Scalable processor to get the optimal performance and take advantage of the built in Intel® Deep Learning Boost (Intel® DL Boost) and BFLOAT16(BF16) extension functionality.</p>
</section>
<section id="required-software">
<h2><a class="toc-backref" href="#id3">Required Software</a><a class="headerlink" href="#required-software" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>Install Ubuntu* 20.04 Linux OS on your host system</p></li>
<li><p>Install Docker* Engine</p></li>
</ul>
</section>
<section id="steps">
<h2><a class="toc-backref" href="#id4">Steps</a><a class="headerlink" href="#steps" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>Download the DLRS v0.7.0 Image and launch it in interactive mode</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker pull sysstacks/dlrs-tensorflow2-ubuntu:v0.7.0-intel-tf
docker run -it --shm-size 8g --security-opt <span class="nv">seccomp</span><span class="o">=</span>unconfined sysstacks/dlrs-tensorflow2-ubuntu:v0.7.0-intel-tf bash
</pre></div>
</div>
</li>
<li><p>At the container shell prompt, run the following command to install required tools</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apt-get update
apt-get install wget unzip git
</pre></div>
</div>
</li>
</ol>
</section>
<section id="run-bert-fine-tune-training-with-the-squad-1-1-data-set">
<h2><a class="toc-backref" href="#id5">Run BERT Fine-tune training with the Squad 1.1 data set</a><a class="headerlink" href="#run-bert-fine-tune-training-with-the-squad-1-1-data-set" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>Download wwm_cased_L-24_H-1024_A-16.zip, which contains a pre-trained Bert model with 24 layers, 1024 hidden units, and 16 attention heads. The model has been trained with the whole word masked using a wordpiece tokenized by Google.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip -P /tmp
unzip /tmp/wwm_cased_L-24_H-1024_A-16.zip -d /tmp
</pre></div>
</div>
</li>
<li><p>Download the Squad 1.1 dataset. This reading comprehension dataset consists of questions posed on a set of Wikipedia articles, where the answer to every question is in the corresponding passage. SQuAD 1.1 contains 100,000+ question-answer pairs on 500+ articles</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json -P /tmp
wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -P /tmp
wget https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py -P /tmp
</pre></div>
</div>
</li>
<li><p>Download the Model Zoo for Intel® Architecture. The Intel Model Zoo* contains links to pre-trained models, sample scripts, best practices, and step-by-step tutorials for many popular open-source machine learning models optimized by Intel to run on Intel® Xeon® Scalable processors. We use the Model Zoo script to run the BERT Fine-tune training and Inference</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone -b v1.6.1 https://github.com/IntelAI/models /tmp/models
</pre></div>
</div>
</li>
<li><p>Change PYTHONPATH to include model zoo benchmark directory</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$PYTHONPATH</span>:/tmp/models/benchmarks
</pre></div>
</div>
</li>
<li><p>Run Bert fine-tune training with Squad 1.1 data set.  Note that these parameters are subject to change according to your hardware requirements.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 /tmp/models/benchmarks/launch_benchmark.py <span class="se">\</span>
--model-name<span class="o">=</span>bert_large <span class="se">\</span>
--precision<span class="o">=</span>bfloat16 <span class="se">\</span>
--mode<span class="o">=</span>training <span class="se">\</span>
--mpi_num_processes<span class="o">=</span><span class="m">4</span> <span class="se">\</span>
--mpi_num_processes_per_socket<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
--num-intra-threads<span class="o">=</span><span class="m">22</span> <span class="se">\</span>
--num-inter-threads<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
--output-dir<span class="o">=</span>/tmp/SQuAD_1.1_fine_tune <span class="se">\</span>
--framework<span class="o">=</span>tensorflow <span class="se">\</span>
--batch-size<span class="o">=</span><span class="m">24</span> <span class="se">\</span>
-- <span class="nv">train_option</span><span class="o">=</span>SQuAD <span class="se">\</span>
<span class="nv">vocab_file</span><span class="o">=</span>/tmp/wwm_cased_L-24_H-1024_A-16/vocab.txt <span class="se">\</span>
<span class="nv">config_file</span><span class="o">=</span>/tmp/wwm_cased_L-24_H-1024_A-16/bert_config.json <span class="se">\</span>
<span class="nv">init_checkpoint</span><span class="o">=</span>/tmp/wwm_cased_L-24_H-1024_A-16/bert_model.ckpt <span class="se">\</span>
<span class="nv">do_train</span><span class="o">=</span>True <span class="se">\</span>
<span class="nv">train_file</span><span class="o">=</span>/tmp/train-v1.1.json <span class="se">\</span>
<span class="nv">do_predict</span><span class="o">=</span>True <span class="se">\</span>
<span class="nv">predict_file</span><span class="o">=</span>/tmp/dev-v1.1.json <span class="se">\</span>
<span class="nv">learning_rate</span><span class="o">=</span><span class="m">1</span>.5e-5  <span class="se">\</span>
<span class="nv">num_train_epochs</span><span class="o">=</span><span class="m">2</span> <span class="se">\</span>
warmup-steps<span class="o">=</span><span class="m">0</span> <span class="se">\</span>
<span class="nv">doc_stride</span><span class="o">=</span><span class="m">128</span> <span class="se">\</span>
<span class="nv">do_lower_case</span><span class="o">=</span>False <span class="se">\</span>
<span class="nv">max_seq_length</span><span class="o">=</span><span class="m">384</span> <span class="se">\</span>
<span class="nv">experimental_gelu</span><span class="o">=</span>True <span class="se">\</span>
optimized-softmax<span class="o">=</span>True <span class="se">\</span>
<span class="nv">mpi_workers_sync_gradients</span><span class="o">=</span>True
</pre></div>
</div>
</li>
<li><p>Run BERT Inference with the Squad 1.1 data set</p>
<ul>
<li><p>Download the pre-trained BERT Large Model</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip -P /tmp
unzip /tmp/wwm_uncased_L-24_H-1024_A-16.zip -d /tmp
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>For BERT Inference, we will use the Intel optimized check point directly</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6_1/bert_large_checkpoints.zip -P /tmp
unzip /tmp/bert_large_checkpoints.zip -d /tmp
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</li>
<li><p>Download the Squad 1.1 data set to the BERT Large Model directory</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -P /tmp/wwm_uncased_L-24_H-1024_A-16
</pre></div>
</div>
</li>
<li><p>Run BERT Inference with following command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>numactl --localalloc --physcpubind<span class="o">=</span><span class="m">0</span>-27 python3 /tmp/models/benchmarks/launch_benchmark.py <span class="se">\</span>
 --model-name<span class="o">=</span>bert_large <span class="se">\</span>
 --precision<span class="o">=</span>bfloat16 <span class="se">\</span>
 --mode<span class="o">=</span>inference <span class="se">\</span>
--framework<span class="o">=</span>tensorflow <span class="se">\</span>
--num-inter-threads <span class="m">1</span> <span class="se">\</span>
--num-intra-threads <span class="m">28</span> <span class="se">\</span>
--batch-size<span class="o">=</span><span class="m">32</span> <span class="se">\</span>
--data-location /tmp/wwm_uncased_L-24_H-1024_A-16 <span class="se">\</span>
--checkpoint /tmp/bert_large_checkpoints <span class="se">\</span>
--output-dir /tmp/SQuAD_1.1_inference <span class="se">\</span>
--benchmark-only
</pre></div>
</div>
</li>
</ol>
</section>
<section id="notices-and-disclaimers">
<h2><a class="toc-backref" href="#id6">NOTICES AND DISCLAIMERS</a><a class="headerlink" href="#notices-and-disclaimers" title="Permalink to this headline"></a></h2>
<p>Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex.
Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure.
Your costs and results may vary.
Intel technologies may require enabled hardware, software or service activation.
© Intel Corporation.  Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and brands may be claimed as the property of others.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dlrs.html" class="btn btn-neutral float-left" title="Deep Learning Reference Stack Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tensorflow/README.html" class="btn btn-neutral float-right" title="Deep Learning Reference Stack with Tensorflow and Intel® oneAPI Deep Neural Network Library (oneDNN)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>