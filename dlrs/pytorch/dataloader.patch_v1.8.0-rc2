diff --git a/torch/utils/data/_utils/worker.py b/torch/utils/data/_utils/worker.py
index a8ca66057b..8ff4f60d92 100644
--- a/torch/utils/data/_utils/worker.py
+++ b/torch/utils/data/_utils/worker.py
@@ -7,11 +7,12 @@ static methods.
 import torch
 import random
 import os
-from dataclasses import dataclass
+from collections import namedtuple
 from torch._six import queue
 from torch._utils import ExceptionWrapper
 from typing import Union
 from . import signal_handling, MP_STATUS_CHECK_INTERVAL, IS_WINDOWS
+import psutil
 
 if IS_WINDOWS:
     import ctypes
@@ -110,14 +111,10 @@ def get_worker_info():
 
 
 r"""Dummy class used to signal the end of an IterableDataset"""
-@dataclass(frozen=True)
-class _IterableDatasetStopIteration(object):
-    worker_id: int
+_IterableDatasetStopIteration = namedtuple('_IterableDatasetStopIteration', ['worker_id'])
 
 r"""Dummy class used to resume the fetching when worker reuse is enabled"""
-@dataclass(frozen=True)
-class _ResumeIteration(object):
-    pass
+_ResumeIteration = namedtuple('_ResumeIteration', [])
 
 def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,
                  auto_collation, collate_fn, drop_last, seed, init_fn, worker_id,
@@ -169,6 +166,8 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,
         iteration_end = False
 
         watchdog = ManagerWatchdog()
+        p = psutil.Process()
+        affinity = p.cpu_affinity()
 
         while watchdog.is_alive():
             try:
@@ -177,7 +176,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,
                 continue
             if isinstance(r, _ResumeIteration):
                 # Acknowledge the main process
-                data_queue.put((r, None))
+                data_queue.put(r)
                 iteration_end = False
                 # Recreate the fetcher for worker-reuse policy
                 fetcher = _DatasetKind.create_fetcher(
@@ -199,6 +198,7 @@ def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,
                 init_exception = None
             else:
                 try:
+                    p.cpu_affinity([affinity[worker_id % len(affinity)]])
                     data = fetcher.fetch(index)
                 except Exception as e:
                     if isinstance(e, StopIteration) and dataset_kind == _DatasetKind.Iterable:
diff --git a/torch/utils/data/dataloader.py b/torch/utils/data/dataloader.py
index a5eeeec671..13b4570ec6 100644
--- a/torch/utils/data/dataloader.py
+++ b/torch/utils/data/dataloader.py
@@ -5,17 +5,18 @@ functions to be run in multiprocessing. E.g., the data loading worker loop is
 in `./_utils/worker.py`.
 """
 
-import os
 import threading
 import itertools
 import warnings
 from typing import Any, Callable, TypeVar, Generic, Sequence, List, Optional
 
+
 import multiprocessing as python_multiprocessing
 import torch
 import torch.multiprocessing as multiprocessing
 from torch._utils import ExceptionWrapper
 from torch._six import queue, string_classes
+import psutil
 
 from . import IterableDataset, Sampler, SequentialSampler, RandomSampler, BatchSampler, Dataset
 from . import _utils
@@ -55,7 +56,7 @@ class _InfiniteConstantSampler(Sampler):
     r"""Analogous to ``itertools.repeat(None, None)``.
     Used as sampler for :class:`~torch.utils.data.IterableDataset`.
 
-    Args:
+    Arguments:
         data_source (Dataset): dataset to sample from
     """
 
@@ -78,7 +79,7 @@ class DataLoader(Generic[T_co]):
 
     See :py:mod:`torch.utils.data` documentation page for more details.
 
-    Args:
+    Arguments:
         dataset (Dataset): dataset from which to load the data.
         batch_size (int, optional): how many samples per batch to load
             (default: ``1``).
@@ -110,11 +111,11 @@ class DataLoader(Generic[T_co]):
         worker_init_fn (callable, optional): If not ``None``, this will be called on each
             worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as
             input, after seeding and before data loading. (default: ``None``)
-        prefetch_factor (int, optional, keyword-only arg): Number of samples loaded
+        prefetch_factor (int, optional, keyword-only arg): Number of sample loaded
             in advance by each worker. ``2`` means there will be a total of
             2 * num_workers samples prefetched across all workers. (default: ``2``)
         persistent_workers (bool, optional): If ``True``, the data loader will not shutdown
-            the worker processes after a dataset has been consumed once. This allows to
+            the worker processes after a dataset has been consumed once. This allows to 
             maintain the workers `Dataset` instances alive. (default: ``False``)
 
 
@@ -140,9 +141,6 @@ class DataLoader(Generic[T_co]):
                  See `Dataset Types`_ for more details on these two types of datasets and how
                  :class:`~torch.utils.data.IterableDataset` interacts with
                  `Multi-process data loading`_.
-
-    .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and
-                 :ref:`data-loading-randomness` notes for random seed related questions.
     """
     dataset: Dataset[T_co]
     batch_size: Optional[int]
@@ -158,7 +156,7 @@ class DataLoader(Generic[T_co]):
     def __init__(self, dataset: Dataset[T_co], batch_size: Optional[int] = 1,
                  shuffle: bool = False, sampler: Optional[Sampler[int]] = None,
                  batch_sampler: Optional[Sampler[Sequence[int]]] = None,
-                 num_workers: int = 0, collate_fn: _collate_fn_t = None,
+                 num_workers: int = 0, blocking: bool = False, collate_fn: _collate_fn_t = None,
                  pin_memory: bool = False, drop_last: bool = False,
                  timeout: float = 0, worker_init_fn: _worker_init_fn_t = None,
                  multiprocessing_context=None, generator=None,
@@ -188,6 +186,7 @@ class DataLoader(Generic[T_co]):
         self.timeout = timeout
         self.worker_init_fn = worker_init_fn
         self.multiprocessing_context = multiprocessing_context
+        self.blocking = blocking
 
         # Arg-check dataset related before checking samplers because we want to
         # tell users that iterable-style datasets are incompatible with custom
@@ -291,13 +290,10 @@ class DataLoader(Generic[T_co]):
 
         self._iterator = None
 
-        self.check_worker_number_rationality()
-
     def _get_iterator(self) -> '_BaseDataLoaderIter':
         if self.num_workers == 0:
             return _SingleProcessDataLoaderIter(self)
         else:
-            self.check_worker_number_rationality()
             return _MultiProcessingDataLoaderIter(self)
 
     @property
@@ -308,6 +304,10 @@ class DataLoader(Generic[T_co]):
     def multiprocessing_context(self, multiprocessing_context):
         if multiprocessing_context is not None:
             if self.num_workers > 0:
+                if not multiprocessing._supports_context:
+                    raise ValueError('multiprocessing_context relies on Python >= 3.4, with '
+                                     'support for different start methods')
+
                 if isinstance(multiprocessing_context, string_classes):
                     valid_start_methods = multiprocessing.get_all_start_methods()
                     if multiprocessing_context not in valid_start_methods:
@@ -399,83 +399,6 @@ class DataLoader(Generic[T_co]):
         else:
             return len(self._index_sampler)
 
-    def check_worker_number_rationality(self):
-        # This function check whether the dataloader's worker number is rational based on
-        # current system's resource. Current rule is that if the number of workers this
-        # Dataloader will create is bigger than the number of logical cpus that is allowed to
-        # use, than we will pop up a warning to let user pay attention.
-        #
-        # eg. If current system has 2 physical CPUs with 16 cores each. And each core support 2
-        #     threads, then the total logical cpus here is 2 * 16 * 2 = 64. Let's say current
-        #     DataLoader process can use half of them which is 32, then the rational max number of
-        #     worker that initiated from this process is 32.
-        #     Now, let's say the created DataLoader has num_works = 40, which is bigger than 32.
-        #     So the warning message is triggered to notify the user to lower the worker number if
-        #     necessary.
-        #
-        #
-        # [Note] Please note that this function repects `cpuset` only when os.sched_getaffinity is
-        #        available (available in most of Linux system, but not OSX and Windows).
-        #        When os.sched_getaffinity is not available, os.cpu_count() is called instead, but
-        #        it doesn't repect cpuset.
-        #        We don't take threading into account since each worker process is single threaded
-        #        at this time.
-        #
-        #        We don't set any threading flags (eg. OMP_NUM_THREADS, MKL_NUM_THREADS, etc)
-        #        other than `torch.set_num_threads` to 1 in the worker process, if the passing
-        #        in functions use 3rd party modules that rely on those threading flags to determine
-        #        how many thread to create (eg. numpy, etc), then it is caller's responsibility to
-        #        set those flags correctly.
-        def _create_warning_msg(num_worker_suggest, num_worker_created, cpuset_checked):
-
-            suggested_max_worker_msg = ((
-                "Our suggested max number of worker in current system is {}{}, which is smaller "
-                "than what this DataLoader is going to create.").format(
-                    num_worker_suggest,
-                    ("" if cpuset_checked else " (`cpuset` is not taken into account)"))
-            ) if num_worker_suggest is not None else (
-                "DataLoader is not able to compute a suggested max number of worker in current system.")
-
-            warn_msg = (
-                "This DataLoader will create {} worker processes in total. {} "
-                "Please be aware that excessive worker creation might get DataLoader running slow or even freeze, "
-                "lower the worker number to avoid potential slowness/freeze if necessary.").format(
-                    num_worker_created,
-                    suggested_max_worker_msg)
-            return warn_msg
-
-        if not self.num_workers or self.num_workers == 0:
-            return
-
-        # try to compute a suggested max number of worker based on system's resource
-        max_num_worker_suggest = None
-        cpuset_checked = False
-        if hasattr(os, 'sched_getaffinity'):
-            try:
-                max_num_worker_suggest = len(os.sched_getaffinity(0))
-                cpuset_checked = True
-            except Exception:
-                pass
-        if max_num_worker_suggest is None:
-            # os.cpu_count() could return Optional[int]
-            # get cpu count first and check None in order to satify mypy check
-            cpu_count = os.cpu_count()
-            if cpu_count is not None:
-                max_num_worker_suggest = cpu_count
-
-        if max_num_worker_suggest is None:
-            warnings.warn(_create_warning_msg(
-                max_num_worker_suggest,
-                self.num_workers,
-                cpuset_checked))
-            return
-
-        if self.num_workers > max_num_worker_suggest:
-            warnings.warn(_create_warning_msg(
-                max_num_worker_suggest,
-                self.num_workers,
-                cpuset_checked))
-
 
 class _BaseDataLoaderIter(object):
     def __init__(self, loader: DataLoader) -> None:
@@ -494,7 +417,6 @@ class _BaseDataLoaderIter(object):
         self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()
         self._persistent_workers = loader.persistent_workers
         self._num_yielded = 0
-        self._profile_name = "enumerate(DataLoader)#{}.__next__".format(self.__class__.__name__)
 
     def __iter__(self) -> '_BaseDataLoaderIter':
         return self
@@ -511,23 +433,22 @@ class _BaseDataLoaderIter(object):
         raise NotImplementedError
 
     def __next__(self) -> Any:
-        with torch.autograd.profiler.record_function(self._profile_name):
-            if self._sampler_iter is None:
-                self._reset()
-            data = self._next_data()
-            self._num_yielded += 1
-            if self._dataset_kind == _DatasetKind.Iterable and \
-                    self._IterableDataset_len_called is not None and \
-                    self._num_yielded > self._IterableDataset_len_called:
-                warn_msg = ("Length of IterableDataset {} was reported to be {} (when accessing len(dataloader)), but {} "
-                            "samples have been fetched. ").format(self._dataset, self._IterableDataset_len_called,
-                                                                  self._num_yielded)
-                if self._num_workers > 0:
-                    warn_msg += ("For multiprocessing data-loading, this could be caused by not properly configuring the "
-                                 "IterableDataset replica at each worker. Please see "
-                                 "https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.")
-                warnings.warn(warn_msg)
-            return data
+        if self._sampler_iter is None:
+            self._reset()
+        data = self._next_data()
+        self._num_yielded += 1
+        if self._dataset_kind == _DatasetKind.Iterable and \
+                self._IterableDataset_len_called is not None and \
+                self._num_yielded > self._IterableDataset_len_called:
+            warn_msg = ("Length of IterableDataset {} was reported to be {} (when accessing len(dataloader)), but {} "
+                        "samples have been fetched. ").format(self._dataset, self._IterableDataset_len_called,
+                                                              self._num_yielded)
+            if self._num_workers > 0:
+                warn_msg += ("For multiprocessing data-loading, this could be caused by not properly configuring the "
+                             "IterableDataset replica at each worker. Please see "
+                             "https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.")
+            warnings.warn(warn_msg)
+        return data
 
     next = __next__  # Python 2 compatibility
 
@@ -616,72 +537,46 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
     #      simple things like acquiring an internal lock of a queue may hang.
     #      Therefore, in this case, we actually need to prevent `__del__` from
     #      being executed, and rely on the automatic termination of daemonic
-    #      children.
-    #
-    #      Thus, we register an `atexit` hook that sets a global flag
+    #      children. Thus, we register an `atexit` hook that sets a global flag
     #      `_utils.python_exit_status`. Since `atexit` hooks are executed in the
     #      reverse order of registration, we are guaranteed that this flag is
-    #      set before library resources we use are freed (which, at least in
-    #      CPython, is done via an `atexit` handler defined in
-    #      `multiprocessing/util.py`
-    #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/util.py#L320-L362
-    #      registered when an object requiring this mechanism is first
-    #      created, e.g., `mp.Queue`
-    #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/context.py#L100-L103
-    #      https://github.com/python/cpython/blob/c606624af8d4cb3b4a052fb263bb983b3f87585b/Lib/multiprocessing/queues.py#L29
-    #      )
-    #
-    #      So in `__del__`, we check if `_utils.python_exit_status` is set or
-    #      `None` (freed), and perform no-op if so.
-    #
-    #      However, simply letting library clean-up codes run can also be bad,
-    #      because such codes (i.e., `multiprocessing.util._exit_function()`)
-    #      include join putting threads for `mp.Queue`, which can be blocking.
-    #      Hence, the main process putting threads are called with
-    #      `cancel_join_thread` at creation.  See later section
-    #      [ 3b. A process won't hang when putting into a queue; ]
-    #      for more details.
+    #      set before library resources we use are freed. (Hooks freeing those
+    #      resources are registered at importing the Python core libraries at
+    #      the top of this file.) So in `__del__`, we check if
+    #      `_utils.python_exit_status` is set or `None` (freed), and perform
+    #      no-op if so.
     #
-    #      Here are two example cases where library clean-up codes can run
-    #      before `__del__` is called:
+    #      Another problem with `__del__` is also related to the library cleanup
+    #      calls. When a process ends, it shuts the all its daemonic children
+    #      down with a SIGTERM (instead of joining them without a timeout).
+    #      Simiarly for threads, but by a different mechanism. This fact,
+    #      together with a few implementation details of multiprocessing, forces
+    #      us to make workers daemonic. All of our problems arise when a
+    #      DataLoader is used in a subprocess, and are caused by multiprocessing
+    #      code which looks more or less like this:
     #
-    #        1. If we hold onto a reference to the iterator, it more often
-    #           than not tries to do `multiprocessing` library cleaning before
-    #           clearing the alive referenced objects (https://github.com/pytorch/pytorch/issues/48666)
-    #           and thus prevents our cleaning-up code to run first.
+    #          try:
+    #              your_function_using_a_dataloader()
+    #          finally:
+    #              multiprocessing.util._exit_function()
     #
-    #        2. A similar issue araises when a `DataLoader` is used in a subprocess.
-    #           When a process ends, it shuts the all its daemonic children
-    #           down with a SIGTERM (instead of joining them without a timeout).
-    #           Simiarly for threads, but by a different mechanism. This fact,
-    #           together with a few implementation details of multiprocessing, forces
-    #           us to make workers daemonic. All of our problems arise when a
-    #           DataLoader is used in a subprocess, and are caused by multiprocessing
-    #           code which looks more or less like this:
+    #      The joining/termination mentioned above happens inside
+    #      `_exit_function()`. Now, if `your_function_using_a_dataloader()`
+    #      throws, the stack trace stored in the exception will prevent the
+    #      frame which uses `DataLoaderIter` to be freed. If the frame has any
+    #      reference to the `DataLoaderIter` (e.g., in a method of the iter),
+    #      its  `__del__`, which starts the shutdown procedure, will not be
+    #      called. That, in turn, means that workers aren't notified. Attempting
+    #      to join in `_exit_function` will then result in a hang.
     #
-    #               try:
-    #                   your_function_using_a_dataloader()
-    #               finally:
-    #                   multiprocessing.util._exit_function()
+    #      For context, `_exit_function` is also registered as an `atexit` call.
+    #      So it is unclear to me (@ssnl) why this is needed in a finally block.
+    #      The code dates back to 2008 and there is no comment on the original
+    #      PEP 371 or patch https://bugs.python.org/issue3050 (containing both
+    #      the finally block and the `atexit` registration) that explains this.
     #
-    #           The joining/termination mentioned above happens inside
-    #           `_exit_function()`. Now, if `your_function_using_a_dataloader()`
-    #           throws, the stack trace stored in the exception will prevent the
-    #           frame which uses `DataLoaderIter` to be freed. If the frame has any
-    #           reference to the `DataLoaderIter` (e.g., in a method of the iter),
-    #           its  `__del__`, which starts the shutdown procedure, will not be
-    #           called. That, in turn, means that workers aren't notified. Attempting
-    #           to join in `_exit_function` will then result in a hang.
-    #
-    #           For context, `_exit_function` is also registered as an `atexit` call.
-    #           So it is unclear to me (@ssnl) why this is needed in a finally block.
-    #           The code dates back to 2008 and there is no comment on the original
-    #           PEP 371 or patch https://bugs.python.org/issue3050 (containing both
-    #           the finally block and the `atexit` registration) that explains this.
-    #
-    #
-    #      Finally, another choice is to just shutdown workers with logic in 1
-    #      above whenever we see an error in `next`. This isn't ideal because
+    #      Another choice is to just shutdown workers with logic in 1 above
+    #      whenever we see an error in `next`. This isn't ideal because
     #        a. It prevents users from using try-catch to resume data loading.
     #        b. It doesn't prevent hanging if users have references to the
     #           iterator.
@@ -729,33 +624,30 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
     #           We use `mp.Queue` which has a separate background thread to put
     #           objects from an unbounded buffer array. The background thread is
     #           daemonic and usually automatically joined when the process
-    #           *exits*.
+    #           exits.
     #
-    #           In case that the receiver has ended abruptly while
-    #           reading from the pipe, the join will hang forever.  The usual
-    #           solution for this in Python is calling  `q.cancel_join_thread`,
-    #           which prevents automatically joining it when finalizing
-    #           (exiting).
+    #           However, in case that the receiver has ended abruptly while
+    #           reading from the pipe, the join will hang forever. Therefore,
+    #           for both `worker_result_queue` (worker -> main process/pin_memory_thread)
+    #           and each `index_queue` (main process -> worker), we use
+    #           `q.cancel_join_thread()` in sender process before any `q.put` to
+    #           prevent this automatic join.
+    #
+    #           Moreover, having all queues called `cancel_join_thread` makes
+    #           implementing graceful shutdown logic in `__del__` much easier.
+    #           It won't need to get from any queue, which would also need to be
+    #           guarded by periodic status checks.
     #
     #           Nonetheless, `cancel_join_thread` must only be called when the
     #           queue is **not** going to be read from or write into by another
     #           process, because it may hold onto a lock or leave corrupted data
     #           in the queue, leading other readers/writers to hang.
     #
-    #           Hence,
-    #             + For worker processes, we only do so (for their output
-    #               queues, i.e., `worker_result_queue`) before exiting.
-    #             + For `pin_memory_thread`, its output queue `data_queue` is a
-    #               `queue.Queue` that does blocking `put` if the queue is full.
-    #               So there is no above problem, but as a result, in
-    #               `_pin_memory_loop`, we do need to  wrap the `put` in a loop
-    #               that breaks not only upon success, but also when the main
-    #               process stops reading, i.e., is shutting down.
-    #             + For loader process, we `cancel_join_thread()` for all
-    #               `_index_queues` because the whole purpose of workers and
-    #               `pin_memory_thread` is to serve the loader process.  If
-    #               loader process is already exiting, we don't really care if
-    #               the queues are corrupted.
+    #           `pin_memory_thread`'s `data_queue` is a `queue.Queue` that does
+    #           a blocking `put` if the queue is full. So there is no above
+    #           problem, but we do need to wrap the `put` in a loop that breaks
+    #           not only upon success, but also when the main process stops
+    #           reading, i.e., is shutting down.
     #
     #
     # Now let's get back to 1:
@@ -891,12 +783,13 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
 
         self._index_queues = []
         self._workers = []
+        self._block_policy = loader.blocking
+        self._batch_size = loader.batch_size
+
         for i in range(self._num_workers):
             # No certainty which module multiprocessing_context is
             index_queue = multiprocessing_context.Queue()  # type: ignore
-            # Need to `cancel_join_thread` here!
-            # See sections (2) and (3b) above.
-            index_queue.cancel_join_thread()
+            # index_queue.cancel_join_thread()
             w = multiprocessing_context.Process(
                 target=_utils.worker._worker_loop,
                 args=(self._dataset_kind, self._dataset, index_queue,
@@ -953,7 +846,7 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
         # contains all `True`s if not using an iterable-style dataset
         # (i.e., if kind != Iterable).
         # Not that this indicates that a worker still has work to do *for this epoch*.
-        # It does not mean that a worker is dead. In case of `_persistent_workers`,
+        # It does not mean that a worker is dead. In case of `_persistent_workers`, 
         # the worker will be reset to available in the next epoch.
         self._workers_status = [True for i in range(self._num_workers)]
         # We resume the prefetching in case it was enabled
@@ -962,13 +855,13 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
                 self._index_queues[idx].put(_utils.worker._ResumeIteration())
             resume_iteration_cnt = self._num_workers
             while resume_iteration_cnt > 0:
-                return_idx, return_data = self._get_data()
-                if isinstance(return_idx, _utils.worker._ResumeIteration):
-                    assert return_data is None
+                data = self._get_data()
+                if isinstance(data, _utils.worker._ResumeIteration):
                     resume_iteration_cnt -= 1
         # prime the prefetch loop
-        for _ in range(self._prefetch_factor * self._num_workers):
-            self._try_put_index()
+        if not self._block_policy:
+            for _ in range(2 * self._num_workers):
+                self._try_put_index()
 
     def _try_get_data(self, timeout=_utils.MP_STATUS_CHECK_INTERVAL):
         # Tries to fetch data from `self._data_queue` once for a given timeout.
@@ -1150,6 +1043,8 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
                     return data
 
     def _next_data(self):
+        if self._block_policy:
+            self._try_put_index()
         while True:
             # If the worker responsible for `self._rcvd_idx` has already ended
             # and was unable to fulfill this task (due to exhausting an `IterableDataset`),
@@ -1174,7 +1069,7 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
             # Now `self._rcvd_idx` is the batch index we want to fetch
 
             # Check if the next sample has already been generated
-            if len(self._task_info[self._rcvd_idx]) == 2:
+            if len(self._task_info[self._rcvd_idx]) == 2 and not self._block_policy:
                 data = self._task_info.pop(self._rcvd_idx)[1]
                 return self._process_data(data)
 
@@ -1191,26 +1086,75 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
                     self._try_put_index()
                     continue
 
-            if idx != self._rcvd_idx:
-                # store out-of-order samples
-                self._task_info[idx] += (data,)
-            else:
-                del self._task_info[idx]
+            # store out-of-order samples
+            self._task_info[idx] += (data,)
+            if idx == self._rcvd_idx and not self._block_policy:
+                 del self._task_info[idx]
+                 return self._process_data(data)
+            elif self._tasks_outstanding == 0 and self._block_policy:
+                img = []
+                label = []
+                for i in range(self._rcvd_idx, self._send_idx):
+                    info = self._task_info[i]
+                    _data = self._task_info.pop(i)[1]
+                    img.append(_data[0])
+                    label.append(_data[1])
+                    self._rcvd_idx += 1
+                data = [torch.cat(img, dim=0), torch.cat(label, dim=0)]
                 return self._process_data(data)
 
     def _try_put_index(self):
-        assert self._tasks_outstanding < self._prefetch_factor * self._num_workers
-
+        if self._block_policy:
+            assert self._tasks_outstanding < self._num_workers
+        else:
+            assert self._tasks_outstanding < 2 * self._num_workers
         try:
             index = self._next_index()
         except StopIteration:
             return
+        available_workers = 0
         for _ in range(self._num_workers):  # find the next active worker, if any
             worker_queue_idx = next(self._worker_queue_idx_cycle)
             if self._workers_status[worker_queue_idx]:
-                break
+                if self._block_policy:
+                    available_workers += 1
+                else:
+                    break
         else:
             # not found (i.e., didn't break)
+            if self._block_policy:
+               if self._batch_size < available_workers:
+                   available_workers = self._batch_size
+               start = 0
+               div = self._batch_size // available_workers
+               rem = self._batch_size % available_workers
+               print("drls logging")
+               print("drls logging")
+               print("drls logging")
+               print("drls logging")
+               Print("bs, div, rem:", self._batch_size, div, rem)
+               print("drls logging")
+               print("drls logging")
+               print("drls logging")
+               print("drls logging")
+               for _ in range(self._num_workers):  # find the next active worker, if any
+                   if start == len(index):
+                       break
+                   worker_queue_idx = next(self._worker_queue_idx_cycle)
+                   if self._workers_status[worker_queue_idx]:
+                       if rem != 0:
+                           end = start + div + 1
+                           rem -= 1
+                       else:
+                           end = start + div
+                       if end > self._batch_size:
+                           end = self._batch_size
+                       idx = index[start: end]
+                       start = end
+                       self._index_queues[worker_queue_idx].put((self._send_idx, idx))
+                       self._task_info[self._send_idx] = (worker_queue_idx,)
+                       self._tasks_outstanding += 1
+                       self._send_idx += 1
             return
 
         self._index_queues[worker_queue_idx].put((self._send_idx, index))
@@ -1219,8 +1163,9 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
         self._send_idx += 1
 
     def _process_data(self, data):
-        self._rcvd_idx += 1
-        self._try_put_index()
+        if not self._block_policy:
+            self._try_put_index()
+            self._rcvd_idx += 1
         if isinstance(data, ExceptionWrapper):
             data.reraise()
         return data
@@ -1264,9 +1209,6 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
         if not self._shutdown:
             self._shutdown = True
             try:
-                # Normal exit when last reference is gone / iterator is depleted.
-                # See (1) and the second half of the note.
-
                 # Exit `pin_memory_thread` first because exiting workers may leave
                 # corrupted data in `worker_result_queue` which `pin_memory_thread`
                 # reads from.
@@ -1291,10 +1233,13 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
                     if self._persistent_workers or self._workers_status[worker_id]:
                         self._mark_worker_as_unavailable(worker_id, shutdown=True)
                 for w in self._workers:
-                    # We should be able to join here, but in case anything went
-                    # wrong, we set a timeout and if the workers fail to join,
-                    # they are killed in the `finally` block.
                     w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
+                    if w.is_alive():
+                        # Existing mechanisms try to make the workers exit
+                        # peacefully, but in case that we unfortunately reach
+                        # here, which we shouldn't, (e.g., pytorch/pytorch#39570),
+                        # we kill the worker.
+                        w.terminate()
                 for q in self._index_queues:
                     q.cancel_join_thread()
                     q.close()
@@ -1312,13 +1257,6 @@ class _MultiProcessingDataLoaderIter(_BaseDataLoaderIter):
                 if self._worker_pids_set:
                     _utils.signal_handling._remove_worker_pids(id(self))
                     self._worker_pids_set = False
-                for w in self._workers:
-                    if w.is_alive():
-                        # Existing mechanisms try to make the workers exit
-                        # peacefully, but in case that we unfortunately reach
-                        # here, which we shouldn't, (e.g., pytorch/pytorch#39570),
-                        # we kill the worker.
-                        w.terminate()
 
     def __del__(self):
         self._shutdown_workers()
